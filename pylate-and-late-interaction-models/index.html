<!doctype html><html lang=en-US><head><meta http-equiv=X-Clacks-Overhead content="GNU Terry Pratchett"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" href=https://martimchaves.github.io/images/favicon.png><title>PyLate and late interaction models | low throughput thought stream</title><meta name=title content="PyLate and late interaction models"><meta name=description content="Late interaction models basically means a model that doesn&rsquo;t compress several embedding vectors into a single vector. So instead of having just one embedding vector for a bit of text, you have N.
This way, when comparing between a query and a document, less context is missed/destroyed. This has shown great results. This preserves context and nuance, improving retrieval accuracy.
The major downside is that it&rsquo;s compute intensive - after all instead of searching for just one embedding vector, we have to compare between groups of N embedding vectors."><meta name=keywords content="LLM,IR,"><meta property="og:url" content="https://martimchaves.github.io/pylate-and-late-interaction-models/"><meta property="og:site_name" content="low throughput thought stream"><meta property="og:title" content="PyLate and late interaction models"><meta property="og:description" content="Late interaction models basically means a model that doesn’t compress several embedding vectors into a single vector. So instead of having just one embedding vector for a bit of text, you have N.
This way, when comparing between a query and a document, less context is missed/destroyed. This has shown great results. This preserves context and nuance, improving retrieval accuracy.
The major downside is that it’s compute intensive - after all instead of searching for just one embedding vector, we have to compare between groups of N embedding vectors."><meta property="og:locale" content="en_US"><meta property="og:type" content="article"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-01-30T11:22:13+00:00"><meta property="article:modified_time" content="2025-01-30T11:22:13+00:00"><meta property="article:tag" content="LLM"><meta property="article:tag" content="IR"><meta property="og:image" content="https://martimchaves.github.io/images/share.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://martimchaves.github.io/images/share.png"><meta name=twitter:title content="PyLate and late interaction models"><meta name=twitter:description content="Late interaction models basically means a model that doesn’t compress several embedding vectors into a single vector. So instead of having just one embedding vector for a bit of text, you have N.
This way, when comparing between a query and a document, less context is missed/destroyed. This has shown great results. This preserves context and nuance, improving retrieval accuracy.
The major downside is that it’s compute intensive - after all instead of searching for just one embedding vector, we have to compare between groups of N embedding vectors."><meta itemprop=name content="PyLate and late interaction models"><meta itemprop=description content="Late interaction models basically means a model that doesn’t compress several embedding vectors into a single vector. So instead of having just one embedding vector for a bit of text, you have N.
This way, when comparing between a query and a document, less context is missed/destroyed. This has shown great results. This preserves context and nuance, improving retrieval accuracy.
The major downside is that it’s compute intensive - after all instead of searching for just one embedding vector, we have to compare between groups of N embedding vectors."><meta itemprop=datePublished content="2025-01-30T11:22:13+00:00"><meta itemprop=dateModified content="2025-01-30T11:22:13+00:00"><meta itemprop=wordCount content="90"><meta itemprop=image content="https://martimchaves.github.io/images/share.png"><meta itemprop=keywords content="LLM,IR"><meta name=referrer content="no-referrer-when-downgrade"><style>:root{--width:800px;--font-main:Verdana, sans-serif;--font-secondary:Verdana, sans-serif;--font-scale:1em;--background-color:#fff;--heading-color:#222;--text-color:#444;--link-color:#3273dc;--visited-color:#8b6fcb;--code-background-color:#f2f2f2;--code-color:#222;--blockquote-color:#222}@media(prefers-color-scheme:dark){:root{--background-color:#333;--heading-color:#eee;--text-color:#ddd;--link-color:#8cc2dd;--visited-color:#8b6fcb;--code-background-color:#777;--code-color:#ddd;--blockquote-color:#ccc}}body{font-family:var(--font-secondary);font-size:var(--font-scale);margin:auto;padding:20px;max-width:var(--width);text-align:left;background-color:var(--background-color);word-wrap:break-word;overflow-wrap:break-word;line-height:1.5;color:var(--text-color)}h1,h2,h3,h4,h5,h6{font-family:var(--font-main);color:var(--heading-color)}a{color:var(--link-color);cursor:pointer;text-decoration:none}a:hover{text-decoration:underline}nav a{margin-right:8px}strong,b{color:var(--heading-color)}button{margin:0;cursor:pointer}main{line-height:1.6}table{width:100%}hr{border:0;border-top:1px dashed}img{max-width:100%}code{font-family:monospace;padding:2px;background-color:var(--code-background-color);color:var(--code-color);border-radius:3px}blockquote{border-left:1px solid #999;color:var(--code-color);padding-left:20px;font-style:italic}footer{padding:25px 0;text-align:center}.title:hover{text-decoration:none}.title h1{font-size:1.5em}.inline{width:auto !important}.highlight,.code{padding:1px 15px;background-color:var(--code-background-color);color:var(--code-color);border-radius:3px;margin-block-start:1em;margin-block-end:1em;overflow-x:auto}ul.blog-posts{list-style-type:none;padding:unset}ul.blog-posts li{display:flex}ul.blog-posts li span{flex:0 0 130px}ul.blog-posts li a:visited{color:var(--visited-color)}</style></head><body><header><a href=/ class=title><h1>low throughput thought stream</h1><h3>by martim chaves</h3></a><nav><a href=/>home</a>
<a href=/blog>blog</a></nav></header><main><h1>PyLate and late interaction models</h1><p><i><time datetime=2025-01-30>30 Jan, 2025</time></i></p><content><p>Late interaction models basically means a model that doesn&rsquo;t compress several embedding vectors into a single vector. So instead of having just one embedding vector for a bit of text, you have N.</p><p>This way, when comparing between a query and a document, less context is missed/destroyed. This has shown great results. This preserves context and nuance, improving retrieval accuracy.</p><p>The major downside is that it&rsquo;s compute intensive - after all instead of searching for just one embedding vector, we have to compare between groups of N embedding vectors.</p><p>TBC&mldr;</p></content><p><a href=https://martimchaves.github.io/blog/llm/>#LLM</a>
<a href=https://martimchaves.github.io/blog/ir/>#IR</a></p></main><footer>made with <a href=https://github.com/janraasch/hugo-bearblog/>Hugo ʕ•ᴥ•ʔ Bear</a></footer></body></html>