<!doctype html><html lang=en-US><head><meta http-equiv=X-Clacks-Overhead content="GNU Terry Pratchett"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" href=https://martimchaves.github.io/images/favicon.png><title>BEIR and FreshStack | low throughput thought stream</title><meta name=title content="BEIR and FreshStack"><meta name=description content="BEIR

The BEIR benchmark (Benchmarking Information Retrieval) is a collection of 19 datasets. Each dataset tests information retrieval for a specific task (question answering, fact-checking, &mldr;).
It was supposedly a zero-shot benchmark - models wouldn&rsquo;t use BEIR to train, so their results on BEIR would be more reliable.
But nowadays, that doesn&rsquo;t really happen - after all, there are some private models, and even with open models, you can, indirectly, tune for a high BEIR score (i.e. keep choosing the best performing strategies for BEIR and improve those).
The scoring algorithm checks if the top 10 retrieved documents are, in order, the best document to answer the query.

The algo is called nDCG@10 (Normalized Discounted Cumulative Gain at 10). How it works:

DCG@10 = sum for positions 1 to 10:

sum relevance of i / log⁡2(i+1) for i in top 10 retrieved documents;
e.g. if ranking is:

Doc A (rel = 2) at pos 1,
Doc B (rel = 1) at pos 2,
others zero:

DCG = 2 / log₂(2) + 1 / log₂(3) ≈ 2 / 1 + 1 / 1.585 ≈ 2 + 0.63 = 2.63






IDCG@10 = the ideal (best possible) DCG using perfect ordering by relevance. If only those two relevant docs, same order gives IDCG ≈ 2.63.
nDCG@10 = DCG@10 / IDCG@10. In this case: 2.63 / 2.63 = 1.0 (perfect score).





FreshStack
FreshStack was created as a framework to create tailored benchmarks for RAG. It works by:"><meta name=keywords content="LLM,IR,"><meta property="og:url" content="https://martimchaves.github.io/beir-and-freshstack/"><meta property="og:site_name" content="low throughput thought stream"><meta property="og:title" content="BEIR and FreshStack"><meta property="og:description" content="BEIR The BEIR benchmark (Benchmarking Information Retrieval) is a collection of 19 datasets. Each dataset tests information retrieval for a specific task (question answering, fact-checking, …). It was supposedly a zero-shot benchmark - models wouldn’t use BEIR to train, so their results on BEIR would be more reliable. But nowadays, that doesn’t really happen - after all, there are some private models, and even with open models, you can, indirectly, tune for a high BEIR score (i.e. keep choosing the best performing strategies for BEIR and improve those). The scoring algorithm checks if the top 10 retrieved documents are, in order, the best document to answer the query. The algo is called nDCG@10 (Normalized Discounted Cumulative Gain at 10). How it works: DCG@10 = sum for positions 1 to 10: sum relevance of i / log⁡2(i+1) for i in top 10 retrieved documents; e.g. if ranking is: Doc A (rel = 2) at pos 1, Doc B (rel = 1) at pos 2, others zero: DCG = 2 / log₂(2) + 1 / log₂(3) ≈ 2 / 1 + 1 / 1.585 ≈ 2 + 0.63 = 2.63 IDCG@10 = the ideal (best possible) DCG using perfect ordering by relevance. If only those two relevant docs, same order gives IDCG ≈ 2.63. nDCG@10 = DCG@10 / IDCG@10. In this case: 2.63 / 2.63 = 1.0 (perfect score). FreshStack FreshStack was created as a framework to create tailored benchmarks for RAG. It works by:"><meta property="og:locale" content="en_US"><meta property="og:type" content="article"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-01-30T11:22:13+00:00"><meta property="article:modified_time" content="2025-01-30T11:22:13+00:00"><meta property="article:tag" content="LLM"><meta property="article:tag" content="IR"><meta property="og:image" content="https://martimchaves.github.io/images/share.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://martimchaves.github.io/images/share.png"><meta name=twitter:title content="BEIR and FreshStack"><meta name=twitter:description content="BEIR The BEIR benchmark (Benchmarking Information Retrieval) is a collection of 19 datasets. Each dataset tests information retrieval for a specific task (question answering, fact-checking, …). It was supposedly a zero-shot benchmark - models wouldn’t use BEIR to train, so their results on BEIR would be more reliable. But nowadays, that doesn’t really happen - after all, there are some private models, and even with open models, you can, indirectly, tune for a high BEIR score (i.e. keep choosing the best performing strategies for BEIR and improve those). The scoring algorithm checks if the top 10 retrieved documents are, in order, the best document to answer the query. The algo is called nDCG@10 (Normalized Discounted Cumulative Gain at 10). How it works: DCG@10 = sum for positions 1 to 10: sum relevance of i / log⁡2(i+1) for i in top 10 retrieved documents; e.g. if ranking is: Doc A (rel = 2) at pos 1, Doc B (rel = 1) at pos 2, others zero: DCG = 2 / log₂(2) + 1 / log₂(3) ≈ 2 / 1 + 1 / 1.585 ≈ 2 + 0.63 = 2.63 IDCG@10 = the ideal (best possible) DCG using perfect ordering by relevance. If only those two relevant docs, same order gives IDCG ≈ 2.63. nDCG@10 = DCG@10 / IDCG@10. In this case: 2.63 / 2.63 = 1.0 (perfect score). FreshStack FreshStack was created as a framework to create tailored benchmarks for RAG. It works by:"><meta itemprop=name content="BEIR and FreshStack"><meta itemprop=description content="BEIR The BEIR benchmark (Benchmarking Information Retrieval) is a collection of 19 datasets. Each dataset tests information retrieval for a specific task (question answering, fact-checking, …). It was supposedly a zero-shot benchmark - models wouldn’t use BEIR to train, so their results on BEIR would be more reliable. But nowadays, that doesn’t really happen - after all, there are some private models, and even with open models, you can, indirectly, tune for a high BEIR score (i.e. keep choosing the best performing strategies for BEIR and improve those). The scoring algorithm checks if the top 10 retrieved documents are, in order, the best document to answer the query. The algo is called nDCG@10 (Normalized Discounted Cumulative Gain at 10). How it works: DCG@10 = sum for positions 1 to 10: sum relevance of i / log⁡2(i+1) for i in top 10 retrieved documents; e.g. if ranking is: Doc A (rel = 2) at pos 1, Doc B (rel = 1) at pos 2, others zero: DCG = 2 / log₂(2) + 1 / log₂(3) ≈ 2 / 1 + 1 / 1.585 ≈ 2 + 0.63 = 2.63 IDCG@10 = the ideal (best possible) DCG using perfect ordering by relevance. If only those two relevant docs, same order gives IDCG ≈ 2.63. nDCG@10 = DCG@10 / IDCG@10. In this case: 2.63 / 2.63 = 1.0 (perfect score). FreshStack FreshStack was created as a framework to create tailored benchmarks for RAG. It works by:"><meta itemprop=datePublished content="2025-01-30T11:22:13+00:00"><meta itemprop=dateModified content="2025-01-30T11:22:13+00:00"><meta itemprop=wordCount content="377"><meta itemprop=image content="https://martimchaves.github.io/images/share.png"><meta itemprop=keywords content="LLM,IR"><meta name=referrer content="no-referrer-when-downgrade"><style>:root{--width:800px;--font-main:Verdana, sans-serif;--font-secondary:Verdana, sans-serif;--font-scale:1em;--background-color:#fff;--heading-color:#222;--text-color:#444;--link-color:#3273dc;--visited-color:#8b6fcb;--code-background-color:#f2f2f2;--code-color:#222;--blockquote-color:#222}@media(prefers-color-scheme:dark){:root{--background-color:#333;--heading-color:#eee;--text-color:#ddd;--link-color:#8cc2dd;--visited-color:#8b6fcb;--code-background-color:#777;--code-color:#ddd;--blockquote-color:#ccc}}body{font-family:var(--font-secondary);font-size:var(--font-scale);margin:auto;padding:20px;max-width:var(--width);text-align:left;background-color:var(--background-color);word-wrap:break-word;overflow-wrap:break-word;line-height:1.5;color:var(--text-color)}h1,h2,h3,h4,h5,h6{font-family:var(--font-main);color:var(--heading-color)}a{color:var(--link-color);cursor:pointer;text-decoration:none}a:hover{text-decoration:underline}nav a{margin-right:8px}strong,b{color:var(--heading-color)}button{margin:0;cursor:pointer}main{line-height:1.6}table{width:100%}hr{border:0;border-top:1px dashed}img{max-width:100%}code{font-family:monospace;padding:2px;background-color:var(--code-background-color);color:var(--code-color);border-radius:3px}blockquote{border-left:1px solid #999;color:var(--code-color);padding-left:20px;font-style:italic}footer{padding:25px 0;text-align:center}.title:hover{text-decoration:none}.title h1{font-size:1.5em}.inline{width:auto !important}.highlight,.code{padding:1px 15px;background-color:var(--code-background-color);color:var(--code-color);border-radius:3px;margin-block-start:1em;margin-block-end:1em;overflow-x:auto}ul.blog-posts{list-style-type:none;padding:unset}ul.blog-posts li{display:flex}ul.blog-posts li span{flex:0 0 130px}ul.blog-posts li a:visited{color:var(--visited-color)}</style></head><body><header><a href=/ class=title><h1>low throughput thought stream</h1><h3>by martim chaves</h3></a><nav><a href=/>home</a>
<a href=/blog>blog</a></nav></header><main><h1>BEIR and FreshStack</h1><p><i><time datetime=2025-01-30>30 Jan, 2025</time></i></p><content><h3 id=beir>BEIR</h3><ul><li>The BEIR benchmark (Benchmarking Information Retrieval) is a collection of 19 datasets. Each dataset tests information retrieval for a specific task (question answering, fact-checking, &mldr;).</li><li>It was supposedly a zero-shot benchmark - models wouldn&rsquo;t use BEIR to train, so their results on BEIR would be more reliable.</li><li>But nowadays, that doesn&rsquo;t really happen - after all, there are some private models, and even with open models, you can, indirectly, tune for a high BEIR score (i.e. keep choosing the best performing strategies for BEIR and improve those).</li><li>The scoring algorithm checks if the top 10 retrieved documents are, in order, the best document to answer the query.<ul><li>The algo is called nDCG@10 (Normalized Discounted Cumulative Gain at 10). How it works:<ul><li>DCG@10 = sum for positions 1 to 10:<ul><li>sum relevance of i / log⁡2(i+1) for i in top 10 retrieved documents;</li><li>e.g. if ranking is:<ul><li>Doc A (rel = 2) at pos 1,</li><li>Doc B (rel = 1) at pos 2,</li><li>others zero:<ul><li>DCG = 2 / log₂(2) + 1 / log₂(3) ≈ 2 / 1 + 1 / 1.585 ≈ 2 + 0.63 = 2.63</li></ul></li></ul></li></ul></li><li>IDCG@10 = the ideal (best possible) DCG using perfect ordering by relevance. If only those two relevant docs, same order gives IDCG ≈ 2.63.</li><li>nDCG@10 = DCG@10 / IDCG@10. In this case: 2.63 / 2.63 = 1.0 (perfect score).</li></ul></li></ul></li></ul><h3 id=freshstack>FreshStack</h3><p>FreshStack was created as a framework to create tailored benchmarks for RAG. It works by:</p><ul><li>Getting queries and answers from stackoverflow (selected recent queries and answers from niche topics)</li><li>Transforming each answer into a set of atomic facts - the nuggets</li><li>Getting a corpus of documents from github using several methods that are relevant to those queries</li><li>Use LLM-as-a-judge to determine if the documents retrieved were relevant to support the nuggets</li></ul><p>The three metrics introduced to check the quality of FreshStack pipeline results are:</p><ul><li>Diversity (alpha-nDCG@10): Prioritize retrieving as little documents as possible. Penalizes the retrieval of multiple documents that support the same fact (measure redundancy).</li><li>Grounding (Coverage@20): Make sure that all nuggets are supported. Measures the percentage of unique nuggets supported by the retrieved documents, directly evaluating evidence collection.</li><li>Relevance (Recall@50): Check whether the retrieved documents are on-topic - % of relevant documents in top 50 retrieved documents.</li></ul></content><p><a href=https://martimchaves.github.io/blog/llm/>#LLM</a>
<a href=https://martimchaves.github.io/blog/ir/>#IR</a></p></main><footer>made with <a href=https://github.com/janraasch/hugo-bearblog/>Hugo ʕ•ᴥ•ʔ Bear</a></footer></body></html>