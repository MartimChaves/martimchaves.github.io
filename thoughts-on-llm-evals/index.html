<!doctype html><html lang=en-US><head><meta http-equiv=X-Clacks-Overhead content="GNU Terry Pratchett"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" href=https://martimchaves.github.io/images/favicon.png><title>thoughts on llm evals | low throughput thought stream</title><meta name=title content="thoughts on llm evals"><meta name=description content="MCP stans for Model Context Protocol. It&rsquo;s a protocol that has been developed to standardize tool use.
Some folks started using LLMs to produce json that could be parsed to run a function - i.e. a tool. Say you built a function that looks like this:
def hello_greeting(name: str):
    print(f&#34;Hello, {name}!&#34;)
You would tell your LLM, something like:
Assistant: If you want to run a greeting tool, you can run the following tool:
Tool name: 'hello_greeting'
Arguments:
  - name: string
And then, you hoped that, if there ever was a need to run this tool, the LLM would output something like this:"><meta name=keywords content="LLM,MCP,"><meta property="og:url" content="https://martimchaves.github.io/thoughts-on-llm-evals/"><meta property="og:site_name" content="low throughput thought stream"><meta property="og:title" content="thoughts on llm evals"><meta property="og:description" content="MCP stans for Model Context Protocol. It’s a protocol that has been developed to standardize tool use.
Some folks started using LLMs to produce json that could be parsed to run a function - i.e. a tool. Say you built a function that looks like this:
def hello_greeting(name: str): print(f&#34;Hello, {name}!&#34;) You would tell your LLM, something like:
Assistant: If you want to run a greeting tool, you can run the following tool: Tool name: 'hello_greeting' Arguments: - name: string And then, you hoped that, if there ever was a need to run this tool, the LLM would output something like this:"><meta property="og:locale" content="en_US"><meta property="og:type" content="article"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-08-19T11:22:13+00:00"><meta property="article:modified_time" content="2025-08-19T11:22:13+00:00"><meta property="article:tag" content="LLM"><meta property="article:tag" content="MCP"><meta property="og:image" content="https://martimchaves.github.io/images/share.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://martimchaves.github.io/images/share.png"><meta name=twitter:title content="thoughts on llm evals"><meta name=twitter:description content="MCP stans for Model Context Protocol. It’s a protocol that has been developed to standardize tool use.
Some folks started using LLMs to produce json that could be parsed to run a function - i.e. a tool. Say you built a function that looks like this:
def hello_greeting(name: str): print(f&#34;Hello, {name}!&#34;) You would tell your LLM, something like:
Assistant: If you want to run a greeting tool, you can run the following tool: Tool name: 'hello_greeting' Arguments: - name: string And then, you hoped that, if there ever was a need to run this tool, the LLM would output something like this:"><meta itemprop=name content="thoughts on llm evals"><meta itemprop=description content="MCP stans for Model Context Protocol. It’s a protocol that has been developed to standardize tool use.
Some folks started using LLMs to produce json that could be parsed to run a function - i.e. a tool. Say you built a function that looks like this:
def hello_greeting(name: str): print(f&#34;Hello, {name}!&#34;) You would tell your LLM, something like:
Assistant: If you want to run a greeting tool, you can run the following tool: Tool name: 'hello_greeting' Arguments: - name: string And then, you hoped that, if there ever was a need to run this tool, the LLM would output something like this:"><meta itemprop=datePublished content="2025-08-19T11:22:13+00:00"><meta itemprop=dateModified content="2025-08-19T11:22:13+00:00"><meta itemprop=wordCount content="473"><meta itemprop=image content="https://martimchaves.github.io/images/share.png"><meta itemprop=keywords content="LLM,MCP"><meta name=referrer content="no-referrer-when-downgrade"><style>:root{--width:800px;--font-main:Verdana, sans-serif;--font-secondary:Verdana, sans-serif;--font-scale:1em;--background-color:#fff;--heading-color:#222;--text-color:#444;--link-color:#3273dc;--visited-color:#8b6fcb;--code-background-color:#f2f2f2;--code-color:#222;--blockquote-color:#222}@media(prefers-color-scheme:dark){:root{--background-color:#333;--heading-color:#eee;--text-color:#ddd;--link-color:#8cc2dd;--visited-color:#8b6fcb;--code-background-color:#777;--code-color:#ddd;--blockquote-color:#ccc}}body{font-family:var(--font-secondary);font-size:var(--font-scale);margin:auto;padding:20px;max-width:var(--width);text-align:left;background-color:var(--background-color);word-wrap:break-word;overflow-wrap:break-word;line-height:1.5;color:var(--text-color)}h1,h2,h3,h4,h5,h6{font-family:var(--font-main);color:var(--heading-color)}a{color:var(--link-color);cursor:pointer;text-decoration:none}a:hover{text-decoration:underline}nav a{margin-right:8px}strong,b{color:var(--heading-color)}button{margin:0;cursor:pointer}main{line-height:1.6}table{width:100%}hr{border:0;border-top:1px dashed}img{max-width:100%}code{font-family:monospace;padding:2px;background-color:var(--code-background-color);color:var(--code-color);border-radius:3px}blockquote{border-left:1px solid #999;color:var(--code-color);padding-left:20px;font-style:italic}footer{padding:25px 0;text-align:center}.title:hover{text-decoration:none}.title h1{font-size:1.5em}.inline{width:auto !important}.highlight,.code{padding:1px 15px;background-color:var(--code-background-color);color:var(--code-color);border-radius:3px;margin-block-start:1em;margin-block-end:1em;overflow-x:auto}ul.blog-posts{list-style-type:none;padding:unset}ul.blog-posts li{display:flex}ul.blog-posts li span{flex:0 0 130px}ul.blog-posts li a:visited{color:var(--visited-color)}</style></head><body><header><a href=/ class=title><h1>low throughput thought stream</h1><h3>by martim chaves</h3></a><nav><a href=/>home</a>
<a href=/blog>blog</a></nav></header><main><h1>thoughts on llm evals</h1><p><i><time datetime=2025-08-19>19 Aug, 2025</time></i></p><content><p>MCP stans for Model Context Protocol. It&rsquo;s a protocol that has been developed to standardize tool use.</p><p>Some folks started using LLMs to produce json that could be parsed to run a function - i.e. a tool. Say you built a function that looks like this:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>hello_greeting</span>(name: str):
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Hello, </span><span style=color:#e6db74>{</span>name<span style=color:#e6db74>}</span><span style=color:#e6db74>!&#34;</span>)
</span></span></code></pre></div><p>You would tell your LLM, something like:</p><pre tabindex=0><code>Assistant: If you want to run a greeting tool, you can run the following tool:
Tool name: &#39;hello_greeting&#39;
Arguments:
  - name: string
</code></pre><p>And then, you hoped that, if there ever was a need to run this tool, the LLM would output something like this:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{<span style=color:#960050;background-color:#1e0010>&#39;tool_name&#39;:</span> <span style=color:#960050;background-color:#1e0010>&#39;hello_greeting&#39;,</span> <span style=color:#960050;background-color:#1e0010>&#39;arguments&#39;:{&#39;name&#39;:&#39;world&#39;</span>}<span style=color:#960050;background-color:#1e0010>}</span>
</span></span></code></pre></div><p>You would parse this, and run the hello_greeting function - either passing the output of the function back to the LLM or just printing it out.</p><p>This worked! But it was a bit fragile. You&rsquo;re counting on the LLM to output clean json and to remember tools.</p><p>This was why MCP was invented. MCP requires a client (i.e. the streaming LLM) and a host server.</p><p>The streaming LLM has been rigurously trained to always output text following a format that explains what the words being output are part of - just a bit of text, a tool_call, or a tool_result. So the training data looks like this:</p><pre tabindex=0><code>{ &#34;type&#34;: &#34;text&#34;, &#34;text&#34;: &#34;heya call the greeting tool to greet the world&#34; }
{ &#34;type&#34;: &#34;text&#34;, &#34;text&#34;: &#34;Hey, how are you? Yes, I&#39;ll call the greeting tool!&#34; }
{ &#34;type&#34;: &#34;tool_use&#34;, &#34;name&#34;: &#34;hello_greeting&#34;, &#34;input&#34;: { &#34;name&#34;: &#34;World&#34; } }
{ &#34;type&#34;: &#34;tool_result&#34;, &#34;text&#34;: &#34;Hello, world!&#34; }
{ &#34;type&#34;: &#34;text&#34;, &#34;text&#34;: &#34;Hello, world!&#34; }
</code></pre><p>Now, if an LLM wants to do a tool call, it will stream out something like this:</p><pre tabindex=0><code>{ &#34;type&#34;: &#34;tool_use&#34;, &#34;name&#34;: &#34;hello_greeting&#34;, &#34;input&#34;: { &#34;name&#34;: &#34;World&#34; } }
</code></pre><p>Instead of this being just simply validated by a simple data model, the host server runs several checks to make sure that the function to run this tool exists, the arguments are right, and if all checks out, returns a nicely formated json tool call, something like:</p><pre tabindex=0><code>{
  &#34;jsonrpc&#34;: &#34;2.0&#34;,
  &#34;id&#34;: 42,
  &#34;method&#34;: &#34;callTool&#34;,
  &#34;params&#34;: {
    &#34;name&#34;: &#34;hello_greeting&#34;,
    &#34;arguments&#34;: {
      &#34;name&#34;: &#34;World&#34;
    }
  }
}
</code></pre><p>This host server is another layer that helps deal with the non-determinis associated with LLMs. This way we guarantee that tool calls are made in a more robust way. If, for example, there&rsquo;s some argument missing or the tool is not available, that message can be returned back to the LLM so that it can try again:</p><pre tabindex=0><code>{ &#34;type&#34;: &#34;tool_use&#34;, &#34;name&#34;: &#34;hello_greeting&#34;, &#34;input&#34;: { &#34;greeting_name&#34;: &#34;World&#34; } }
</code></pre><p>beep beep, processing&mldr; woops looks like there&rsquo;s no greeting_name argument!</p><pre tabindex=0><code>{ &#34;type&#34;: &#34;tool_error&#34;, &#34;text&#34;: &#34;greeting_name is not an argument to the hello_greeting tool. Available arguments: name&#34; }
</code></pre><p>And that&rsquo;s why MCP was developed - to standardize a robust way to allow LLMs to interact with tools.</p></content><p><a href=https://martimchaves.github.io/blog/llm/>#LLM</a>
<a href=https://martimchaves.github.io/blog/mcp/>#MCP</a></p></main><footer>made with <a href=https://github.com/janraasch/hugo-bearblog/>Hugo ʕ•ᴥ•ʔ Bear</a></footer></body></html>