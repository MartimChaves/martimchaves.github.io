<!doctype html><html lang=en-US><head><meta http-equiv=X-Clacks-Overhead content="GNU Terry Pratchett"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" href=https://martimchaves.github.io/images/favicon.png><title>Rank1 - IR with reasoning | low throughput thought stream</title><meta name=title content="Rank1 - IR with reasoning"><meta name=description content="
RANK1 is a reranker. It takes in a query and a document and determines the value of the document to answer the query.
It has the architecture of a regular LLM, but it was finetuned with the chain of thought trace of larger reasoning models - DeepSeek R1.
So, even though it&rsquo;s doing next token prediction, it can, in a way, reason about how the document fetched is valuable to answer the query. This makes it more available to capture nuance in the instructions of the query.
After the chain of thought, the model predicts, as a next token, the score, from 0 to 1, of how relevant the document is to the query.

TBC&mldr;"><meta name=keywords content="LLM,IR,"><meta property="og:url" content="https://martimchaves.github.io/rank1-ir-with-reasoning/"><meta property="og:site_name" content="low throughput thought stream"><meta property="og:title" content="Rank1 - IR with reasoning"><meta property="og:description" content="RANK1 is a reranker. It takes in a query and a document and determines the value of the document to answer the query. It has the architecture of a regular LLM, but it was finetuned with the chain of thought trace of larger reasoning models - DeepSeek R1. So, even though it’s doing next token prediction, it can, in a way, reason about how the document fetched is valuable to answer the query. This makes it more available to capture nuance in the instructions of the query. After the chain of thought, the model predicts, as a next token, the score, from 0 to 1, of how relevant the document is to the query. TBC…"><meta property="og:locale" content="en_US"><meta property="og:type" content="article"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-01-30T11:22:13+00:00"><meta property="article:modified_time" content="2025-01-30T11:22:13+00:00"><meta property="article:tag" content="LLM"><meta property="article:tag" content="IR"><meta property="og:image" content="https://martimchaves.github.io/images/share.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://martimchaves.github.io/images/share.png"><meta name=twitter:title content="Rank1 - IR with reasoning"><meta name=twitter:description content="RANK1 is a reranker. It takes in a query and a document and determines the value of the document to answer the query. It has the architecture of a regular LLM, but it was finetuned with the chain of thought trace of larger reasoning models - DeepSeek R1. So, even though it’s doing next token prediction, it can, in a way, reason about how the document fetched is valuable to answer the query. This makes it more available to capture nuance in the instructions of the query. After the chain of thought, the model predicts, as a next token, the score, from 0 to 1, of how relevant the document is to the query. TBC…"><meta itemprop=name content="Rank1 - IR with reasoning"><meta itemprop=description content="RANK1 is a reranker. It takes in a query and a document and determines the value of the document to answer the query. It has the architecture of a regular LLM, but it was finetuned with the chain of thought trace of larger reasoning models - DeepSeek R1. So, even though it’s doing next token prediction, it can, in a way, reason about how the document fetched is valuable to answer the query. This makes it more available to capture nuance in the instructions of the query. After the chain of thought, the model predicts, as a next token, the score, from 0 to 1, of how relevant the document is to the query. TBC…"><meta itemprop=datePublished content="2025-01-30T11:22:13+00:00"><meta itemprop=dateModified content="2025-01-30T11:22:13+00:00"><meta itemprop=wordCount content="115"><meta itemprop=image content="https://martimchaves.github.io/images/share.png"><meta itemprop=keywords content="LLM,IR"><meta name=referrer content="no-referrer-when-downgrade"><style>:root{--width:800px;--font-main:Verdana, sans-serif;--font-secondary:Verdana, sans-serif;--font-scale:1em;--background-color:#fff;--heading-color:#222;--text-color:#444;--link-color:#3273dc;--visited-color:#8b6fcb;--code-background-color:#f2f2f2;--code-color:#222;--blockquote-color:#222}@media(prefers-color-scheme:dark){:root{--background-color:#333;--heading-color:#eee;--text-color:#ddd;--link-color:#8cc2dd;--visited-color:#8b6fcb;--code-background-color:#777;--code-color:#ddd;--blockquote-color:#ccc}}body{font-family:var(--font-secondary);font-size:var(--font-scale);margin:auto;padding:20px;max-width:var(--width);text-align:left;background-color:var(--background-color);word-wrap:break-word;overflow-wrap:break-word;line-height:1.5;color:var(--text-color)}h1,h2,h3,h4,h5,h6{font-family:var(--font-main);color:var(--heading-color)}a{color:var(--link-color);cursor:pointer;text-decoration:none}a:hover{text-decoration:underline}nav a{margin-right:8px}strong,b{color:var(--heading-color)}button{margin:0;cursor:pointer}main{line-height:1.6}table{width:100%}hr{border:0;border-top:1px dashed}img{max-width:100%}code{font-family:monospace;padding:2px;background-color:var(--code-background-color);color:var(--code-color);border-radius:3px}blockquote{border-left:1px solid #999;color:var(--code-color);padding-left:20px;font-style:italic}footer{padding:25px 0;text-align:center}.title:hover{text-decoration:none}.title h1{font-size:1.5em}.inline{width:auto !important}.highlight,.code{padding:1px 15px;background-color:var(--code-background-color);color:var(--code-color);border-radius:3px;margin-block-start:1em;margin-block-end:1em;overflow-x:auto}ul.blog-posts{list-style-type:none;padding:unset}ul.blog-posts li{display:flex}ul.blog-posts li span{flex:0 0 130px}ul.blog-posts li a:visited{color:var(--visited-color)}</style></head><body><header><a href=/ class=title><h1>low throughput thought stream</h1><h3>by martim chaves</h3></a><nav><a href=/>home</a>
<a href=/blog>blog</a></nav></header><main><h1>Rank1 - IR with reasoning</h1><p><i><time datetime=2025-01-30>30 Jan, 2025</time></i></p><content><ul><li>RANK1 is a <strong>reranker</strong>. It takes in a query and a document and determines the value of the document to answer the query.</li><li>It has the architecture of a regular LLM, but it was finetuned with the chain of thought trace of larger reasoning models - DeepSeek R1.</li><li>So, even though it&rsquo;s doing next token prediction, it can, in a way, reason about how the document fetched is valuable to answer the query. This makes it more available to capture nuance in the instructions of the query.</li><li>After the chain of thought, the model predicts, as a next token, the score, from 0 to 1, of how relevant the document is to the query.</li></ul><p>TBC&mldr;</p></content><p><a href=https://martimchaves.github.io/blog/llm/>#LLM</a>
<a href=https://martimchaves.github.io/blog/ir/>#IR</a></p></main><footer>made with <a href=https://github.com/janraasch/hugo-bearblog/>Hugo ʕ•ᴥ•ʔ Bear</a></footer></body></html>