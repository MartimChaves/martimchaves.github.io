<!doctype html><html lang=en-US><head><meta http-equiv=X-Clacks-Overhead content="GNU Terry Pratchett"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" href=https://martimchaves.github.io/images/favicon.png><title>Promptriever - instructions-aware IR | low throughput thought stream</title><meta name=title content="Promptriever - instructions-aware IR"><meta name=description content="Promptriever is an encoding transformer (aka like regular LLM) that has been finetuned to create embeddings that learn instructions for queries used in IR.
The creators of promptriever started with a dataset of pairs of (query, relevant document). From these pairs, they created triplets with a positive instruction (i.e. an instruction that the relevant document respected) and a negative instruction (an instruction that the document didn&rsquo;t respect). Then they finetuned the model to bring the embedding of the [query+positive instruction] closer to the embedding of the relevant document, and the opposite for the negative instruction. This is how they created this model that respects instructions when retrieving documents."><meta name=keywords content="LLM,IR,"><meta property="og:url" content="https://martimchaves.github.io/promptriever-instructions-aware-ir/"><meta property="og:site_name" content="low throughput thought stream"><meta property="og:title" content="Promptriever - instructions-aware IR"><meta property="og:description" content="Promptriever is an encoding transformer (aka like regular LLM) that has been finetuned to create embeddings that learn instructions for queries used in IR.
The creators of promptriever started with a dataset of pairs of (query, relevant document). From these pairs, they created triplets with a positive instruction (i.e. an instruction that the relevant document respected) and a negative instruction (an instruction that the document didn’t respect). Then they finetuned the model to bring the embedding of the [query+positive instruction] closer to the embedding of the relevant document, and the opposite for the negative instruction. This is how they created this model that respects instructions when retrieving documents."><meta property="og:locale" content="en_US"><meta property="og:type" content="article"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-01-30T11:22:13+00:00"><meta property="article:modified_time" content="2025-01-30T11:22:13+00:00"><meta property="article:tag" content="LLM"><meta property="article:tag" content="IR"><meta property="og:image" content="https://martimchaves.github.io/images/share.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://martimchaves.github.io/images/share.png"><meta name=twitter:title content="Promptriever - instructions-aware IR"><meta name=twitter:description content="Promptriever is an encoding transformer (aka like regular LLM) that has been finetuned to create embeddings that learn instructions for queries used in IR.
The creators of promptriever started with a dataset of pairs of (query, relevant document). From these pairs, they created triplets with a positive instruction (i.e. an instruction that the relevant document respected) and a negative instruction (an instruction that the document didn’t respect). Then they finetuned the model to bring the embedding of the [query+positive instruction] closer to the embedding of the relevant document, and the opposite for the negative instruction. This is how they created this model that respects instructions when retrieving documents."><meta itemprop=name content="Promptriever - instructions-aware IR"><meta itemprop=description content="Promptriever is an encoding transformer (aka like regular LLM) that has been finetuned to create embeddings that learn instructions for queries used in IR.
The creators of promptriever started with a dataset of pairs of (query, relevant document). From these pairs, they created triplets with a positive instruction (i.e. an instruction that the relevant document respected) and a negative instruction (an instruction that the document didn’t respect). Then they finetuned the model to bring the embedding of the [query+positive instruction] closer to the embedding of the relevant document, and the opposite for the negative instruction. This is how they created this model that respects instructions when retrieving documents."><meta itemprop=datePublished content="2025-01-30T11:22:13+00:00"><meta itemprop=dateModified content="2025-01-30T11:22:13+00:00"><meta itemprop=wordCount content="211"><meta itemprop=image content="https://martimchaves.github.io/images/share.png"><meta itemprop=keywords content="LLM,IR"><meta name=referrer content="no-referrer-when-downgrade"><style>:root{--width:800px;--font-main:Verdana, sans-serif;--font-secondary:Verdana, sans-serif;--font-scale:1em;--background-color:#fff;--heading-color:#222;--text-color:#444;--link-color:#3273dc;--visited-color:#8b6fcb;--code-background-color:#f2f2f2;--code-color:#222;--blockquote-color:#222}@media(prefers-color-scheme:dark){:root{--background-color:#333;--heading-color:#eee;--text-color:#ddd;--link-color:#8cc2dd;--visited-color:#8b6fcb;--code-background-color:#777;--code-color:#ddd;--blockquote-color:#ccc}}body{font-family:var(--font-secondary);font-size:var(--font-scale);margin:auto;padding:20px;max-width:var(--width);text-align:left;background-color:var(--background-color);word-wrap:break-word;overflow-wrap:break-word;line-height:1.5;color:var(--text-color)}h1,h2,h3,h4,h5,h6{font-family:var(--font-main);color:var(--heading-color)}a{color:var(--link-color);cursor:pointer;text-decoration:none}a:hover{text-decoration:underline}nav a{margin-right:8px}strong,b{color:var(--heading-color)}button{margin:0;cursor:pointer}main{line-height:1.6}table{width:100%}hr{border:0;border-top:1px dashed}img{max-width:100%}code{font-family:monospace;padding:2px;background-color:var(--code-background-color);color:var(--code-color);border-radius:3px}blockquote{border-left:1px solid #999;color:var(--code-color);padding-left:20px;font-style:italic}footer{padding:25px 0;text-align:center}.title:hover{text-decoration:none}.title h1{font-size:1.5em}.inline{width:auto !important}.highlight,.code{padding:1px 15px;background-color:var(--code-background-color);color:var(--code-color);border-radius:3px;margin-block-start:1em;margin-block-end:1em;overflow-x:auto}ul.blog-posts{list-style-type:none;padding:unset}ul.blog-posts li{display:flex}ul.blog-posts li span{flex:0 0 130px}ul.blog-posts li a:visited{color:var(--visited-color)}</style></head><body><header><a href=/ class=title><h1>low throughput thought stream</h1><h3>by martim chaves</h3></a><nav><a href=/>home</a>
<a href=/blog>blog</a></nav></header><main><h1>Promptriever - instructions-aware IR</h1><p><i><time datetime=2025-01-30>30 Jan, 2025</time></i></p><content><p>Promptriever is an encoding transformer (aka like regular LLM) that has been finetuned to create embeddings that learn instructions for queries used in IR.</p><p>The creators of promptriever started with a dataset of pairs of (query, relevant document). From these pairs, they created triplets with a positive instruction (i.e. an instruction that the relevant document respected) and a negative instruction (an instruction that the document didn&rsquo;t respect). Then they finetuned the model to bring the embedding of the [query+positive instruction] closer to the embedding of the relevant document, and the opposite for the negative instruction. This is how they created this model that respects instructions when retrieving documents.</p><p>Example:
Query: What is the capital of France?
Document: Oh lala le capital of France is none other than the beatiful ville of Paris!</p><p>Triplets created:
Query: What is the capital of France?
Positive instruction: In a fun, informal way
Relevant Document: Oh lala le capital of France is none other than the beatiful ville of Paris!
-> pull these embeddings closer</p><p>Query: What is the capital of France?
Negative instruction: In a very formal way
Relevant Document (same document as before): Oh lala le capital of France is none other than the beatiful ville of Paris!
-> pull these embeddings apart</p><p>TBC&mldr;</p></content><p><a href=https://martimchaves.github.io/blog/llm/>#LLM</a>
<a href=https://martimchaves.github.io/blog/ir/>#IR</a></p></main><footer>made with <a href=https://github.com/janraasch/hugo-bearblog/>Hugo ʕ•ᴥ•ʔ Bear</a></footer></body></html>